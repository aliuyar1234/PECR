name: ci

on:
  pull_request:
  push:
    branches:
      - main
      - master

permissions:
  contents: read

jobs:
  quality:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: pecr
          POSTGRES_PASSWORD: pecr
          POSTGRES_DB: pecr
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U pecr -d pecr"
          --health-interval 5s
          --health-timeout 5s
          --health-retries 20
    steps:
      - uses: actions/checkout@v4

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: 1.91.1
          components: rustfmt, clippy

      - name: Cache cargo
        uses: Swatinem/rust-cache@v2

      - name: Ensure scripts executable
        shell: bash
        run: |
          set -euo pipefail
          chmod +x scripts/ci.sh
          chmod +x scripts/perf/suite7.sh

      - name: CI checks (fmt/clippy/test)
        shell: bash
        env:
          PECR_TEST_DB_URL: postgres://pecr:pecr@localhost:5432/pecr
        run: |
          set -euo pipefail
          mkdir -p artifacts
          ./scripts/ci.sh 2>&1 | tee artifacts/ci.log

      - name: Upload quality artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ci-quality-artifacts
          path: artifacts
          if-no-files-found: error

  perf:
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: 1.91.1

      - name: Cache cargo
        uses: Swatinem/rust-cache@v2

      - name: Ensure scripts executable
        shell: bash
        run: |
          set -euo pipefail
          chmod +x scripts/perf/suite7.sh
          chmod +x scripts/perf/compare_k6_baseline.py
          chmod +x scripts/perf/benchmark_matrix.py
          chmod +x scripts/perf/select_median_summary.py
          chmod +x scripts/ops/canary_rollout_guard.py

      - name: Suite 7 baseline + RLM matrix + canary guard
        shell: bash
        env:
          BASELINE_VUS: "10"
          BASELINE_DURATION: "10s"
          FAULT_VUS: "10"
          FAULT_DURATION: "5s"
          COLD_START: "1"
          WAIT_TIMEOUT_SECS: "120"
          RETRY_ATTEMPTS: "4"
          RETRY_SLEEP_SECS: "3"
          COMPOSE_PROJECT_NAME: "pecr-perf-${{ github.run_id }}-${{ github.run_attempt }}"
          CONTROLLER_P99_BUDGET_MS: "1500"
          GATEWAY_P99_BUDGET_MS: "900"
          BVR_THRESHOLD: "0.005"
          SER_THRESHOLD: "0.005"
          PECR_PERF_REGRESSION_P90_FACTOR: "1.8"
          PECR_PERF_REGRESSION_P95_FACTOR: "1.8"
          PECR_PERF_REGRESSION_P99_FACTOR: "1.8"
          PECR_PERF_REGRESSION_ABS_MS: "400"
          PECR_PERF_MIN_RATE_FACTOR: "0.85"
          PECR_PERF_RATE_ABS_DROP: "5"
          PECR_CANARY_P95_BUDGET_MS: "1200"
          PECR_CANARY_P99_BUDGET_MS: "1500"
          PECR_CANARY_SOURCE_UNAVAILABLE_MAX_RATE: "0.05"
          SUITE7_EXPECTATIONS_FILE: "perf/config/suite7_expectations.v1.json"
          SUITE7_EXPECTATIONS_SCHEMA_VERSION: "1"
          SUITE7_ENFORCE_TERMINAL_MODE_ASSERTIONS: "0"
          SUITE7_BASELINE_REPEATS: "3"
        run: |
          set -euo pipefail
          mkdir -p artifacts artifacts/perf artifacts/ops
          overall_status=0

          set +e
          ./scripts/perf/suite7.sh 2>&1 | tee artifacts/suite7.log
          suite7_status=${PIPESTATUS[0]}
          set -e

          cp -a target/perf/. artifacts/perf/ 2>/dev/null || true
          if [[ "${suite7_status}" -ne 0 ]]; then
            overall_status="${suite7_status}"
          fi

          set +e
          PECR_CONTROLLER_ENGINE_OVERRIDE="rlm" \
          PECR_RLM_SANDBOX_ACK="1" \
          SUITE7_SKIP_FAULTS="1" \
          CONTROLLER_BASELINE_SUMMARY_NAME="suite7_rlm_baseline.summary.json" \
          GATEWAY_BASELINE_SUMMARY_NAME="suite7_rlm_gateway_baseline.summary.json" \
          METRICS_GATES_FILE="target/perf/suite7_rlm_metrics_gates.json" \
          ./scripts/perf/suite7.sh 2>&1 | tee artifacts/suite7_rlm.log
          suite7_rlm_status=${PIPESTATUS[0]}
          set -e

          cp -a target/perf/. artifacts/perf/ 2>/dev/null || true
          if [[ "${suite7_rlm_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${suite7_rlm_status}"
          fi

          set +e
          python3 scripts/perf/compare_k6_baseline.py \
            --baseline perf/baselines/suite7_baseline.summary.json \
            --current target/perf/suite7_baseline.summary.json \
            --alarm-label baseline \
            --output-json artifacts/perf/perf_alarm_baseline.json \
            --github-annotations \
            2>&1 | tee artifacts/perf_regression.log
          baseline_alarm_status=${PIPESTATUS[0]}
          set -e

          if [[ "${baseline_alarm_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${baseline_alarm_status}"
          fi

          set +e
          python3 scripts/perf/compare_k6_baseline.py \
            --baseline perf/baselines/suite7_baseline.summary.json \
            --current target/perf/suite7_rlm_baseline.summary.json \
            --alarm-label rlm \
            --output-json artifacts/perf/perf_alarm_rlm.json \
            --github-annotations \
            2>&1 | tee artifacts/perf_regression_rlm.log
          rlm_alarm_status=${PIPESTATUS[0]}
          set -e

          if [[ "${rlm_alarm_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${rlm_alarm_status}"
          fi

          set +e
          python3 scripts/perf/benchmark_matrix.py \
            --baseline perf/baselines/suite7_baseline.summary.json \
            --candidate baseline=target/perf/suite7_baseline.summary.json \
            --candidate rlm=target/perf/suite7_rlm_baseline.summary.json \
            --output-json artifacts/perf/benchmark_matrix.json \
            --output-md artifacts/perf/benchmark_matrix.md \
            --append-step-summary "${GITHUB_STEP_SUMMARY}" \
            --strict \
            2>&1 | tee artifacts/perf_matrix.log
          matrix_status=${PIPESTATUS[0]}
          set -e

          if [[ "${matrix_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${matrix_status}"
          fi

          set +e
          python3 scripts/ops/canary_rollout_guard.py \
            --summary target/perf/suite7_rlm_baseline.summary.json \
            --metrics-gates target/perf/suite7_rlm_metrics_gates.json \
            --engine rlm \
            --adaptive-enabled true \
            --batch-enabled true \
            --output-json artifacts/ops/canary_guard.json \
            --output-md artifacts/ops/canary_guard.md \
            --output-env artifacts/ops/canary_fallback.env \
            --fail-on-fallback \
            --github-annotations \
            2>&1 | tee artifacts/ops/canary_guard.log
          canary_status=${PIPESTATUS[0]}
          set -e

          if [[ "${canary_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${canary_status}"
          fi

          SUITE7_STATUS="${suite7_status}" \
          SUITE7_RLM_STATUS="${suite7_rlm_status}" \
          BASELINE_ALARM_STATUS="${baseline_alarm_status}" \
          RLM_ALARM_STATUS="${rlm_alarm_status}" \
          MATRIX_STATUS="${matrix_status}" \
          CANARY_STATUS="${canary_status}" \
          OVERALL_STATUS="${overall_status}" \
          python3 - <<'PY'
          import json
          import os
          from pathlib import Path

          status_map = {
              "suite7_status": int(os.environ["SUITE7_STATUS"]),
              "suite7_rlm_status": int(os.environ["SUITE7_RLM_STATUS"]),
              "baseline_alarm_status": int(os.environ["BASELINE_ALARM_STATUS"]),
              "rlm_alarm_status": int(os.environ["RLM_ALARM_STATUS"]),
              "matrix_status": int(os.environ["MATRIX_STATUS"]),
              "canary_status": int(os.environ["CANARY_STATUS"]),
          }
          overall_status = int(os.environ["OVERALL_STATUS"])
          labels = {
              "suite7_status": "suite7 baseline execution",
              "suite7_rlm_status": "suite7 rlm execution",
              "baseline_alarm_status": "baseline regression gate",
              "rlm_alarm_status": "rlm regression gate",
              "matrix_status": "benchmark matrix strict gate",
              "canary_status": "canary rollout guard",
          }

          failed = [
              {
                  "stage": key,
                  "label": labels[key],
                  "exit_code": value,
              }
              for key, value in status_map.items()
              if value != 0
          ]
          payload = {
              "overall_status": overall_status,
              "status_map": status_map,
              "failed_stages": failed,
              "first_failed_stage": failed[0]["stage"] if failed else None,
          }

          out_dir = Path("artifacts/perf")
          out_dir.mkdir(parents=True, exist_ok=True)
          (out_dir / "perf_failure_reasons.json").write_text(
              json.dumps(payload, indent=2) + "\n", encoding="utf-8"
          )

          lines = ["### Perf Failure Reasons", ""]
          lines.append(f"- Overall status: `{overall_status}`")
          if failed:
              lines.append("- Failed stages:")
              for row in failed:
                  lines.append(
                      f"  - `{row['stage']}` (`{row['label']}`) exit_code={row['exit_code']}"
                  )
          else:
              lines.append("- Failed stages: none")
          (out_dir / "perf_failure_reasons.md").write_text(
              "\n".join(lines) + "\n", encoding="utf-8"
          )
          print(json.dumps(payload, indent=2))
          PY

          exit "${overall_status}"

      - name: Perf stack cleanup
        if: always()
        shell: bash
        run: |
          set +e
          docker compose down -v --remove-orphans

      - name: Publish perf signal summary
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p artifacts
          baseline_summary_file="target/perf/suite7_baseline.summary.json"
          # shellcheck disable=SC2006

          {
            echo "### Suite 7 Performance Signal (Baseline + RLM)"
            echo
            if [[ -f "${baseline_summary_file}" ]]; then
          python3 - <<'PY'
          import json
          from pathlib import Path

          def print_summary(path_str: str, label: str) -> None:
              path = Path(path_str)
              if not path.exists():
                  print(f"- `{label}` summary missing: `{path_str}`")
                  return
              data = json.loads(path.read_text(encoding="utf-8"))
              metrics = data.get("metrics", {})
              dur = metrics.get("http_req_duration", {})
              p95 = dur.get("p(95)", "n/a")
              p99 = dur.get("p(99)", "n/a")
              print(f"- `{label}` p95: {p95}")
              print(f"- `{label}` p99: {p99}")

          print_summary("target/perf/suite7_baseline.summary.json", "baseline")
          print_summary("target/perf/suite7_rlm_baseline.summary.json", "rlm")
          PY
            else
              echo "- Summary JSON: missing (`target/perf/suite7_baseline.summary.json`)"
              echo "::warning::Missing Suite 7 summary JSON (target/perf/suite7_baseline.summary.json)"
            fi

            if [[ -f artifacts/perf_regression.log ]] && grep -q "perf regression gate: FAIL" artifacts/perf_regression.log; then
              echo "- Baseline regression gate: FAILED (see 'artifacts/perf_regression.log')"
              echo "::error title=Performance regression detected::Baseline regression gate failed. See artifacts/perf_regression.log."
            elif [[ -f artifacts/perf_regression.log ]]; then
              echo "- Baseline regression gate: passed"
              echo "::notice title=Performance regression gate::Baseline regression gate passed."
            else
              echo "- Baseline regression gate: no log produced"
            fi

            if [[ -f artifacts/perf_regression_rlm.log ]] && grep -q "perf regression gate: FAIL" artifacts/perf_regression_rlm.log; then
              echo "- RLM regression gate: FAILED (see 'artifacts/perf_regression_rlm.log')"
              echo "::error title=RLM regression detected::RLM regression gate failed. See artifacts/perf_regression_rlm.log."
            elif [[ -f artifacts/perf_regression_rlm.log ]]; then
              echo "- RLM regression gate: passed"
            else
              echo "- RLM regression gate: no log produced"
            fi

            if [[ -f artifacts/ops/canary_guard.json ]]; then
          python3 - <<'PY'
          import json
          from pathlib import Path

          path = Path("artifacts/ops/canary_guard.json")
          data = json.loads(path.read_text(encoding="utf-8"))
          status = data.get("status", "unknown")
          step = data.get("recommended_fallback", {}).get("step", "none")
          print(f"- Canary guard status: {status} (recommended step: {step})")
          PY
            else
              echo "- Canary guard: artifact missing (`artifacts/ops/canary_guard.json`)"
            fi

            echo "- Alarm artifacts: 'artifacts/perf/perf_alarm_baseline.json', 'artifacts/perf/perf_alarm_rlm.json'"
            echo "- Benchmark matrix: 'artifacts/perf/benchmark_matrix.md'"
            echo "- Structured perf failure reasons: 'artifacts/perf/perf_failure_reasons.json'"
            echo "- Canary fallback env patch: 'artifacts/ops/canary_fallback.env'"
          } | tee artifacts/perf_signal_summary.md >> "${GITHUB_STEP_SUMMARY}"

      - name: Upload perf artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ci-perf-artifacts
          path: artifacts
          if-no-files-found: error

  contracts:
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Contract lock check
        run: python3 scripts/contracts/check_contract_lock.py

  gate:
    if: always()
    needs:
      - quality
      - perf
      - contracts
    runs-on: ubuntu-latest
    steps:
      - name: Enforce all CI gates passed
        run: |
          set -euo pipefail
          test "${{ needs.quality.result }}" = "success"
          test "${{ needs.perf.result }}" = "success"
          test "${{ needs.contracts.result }}" = "success"
