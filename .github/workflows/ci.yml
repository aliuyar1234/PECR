name: ci

on:
  pull_request:
  push:
    branches:
      - main
      - master

permissions:
  contents: read

jobs:
  quality:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: pecr
          POSTGRES_PASSWORD: pecr
          POSTGRES_DB: pecr
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U pecr -d pecr"
          --health-interval 5s
          --health-timeout 5s
          --health-retries 20
    steps:
      - uses: actions/checkout@v4

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: 1.91.1
          components: rustfmt, clippy

      - name: Cache cargo
        uses: Swatinem/rust-cache@v2

      - name: Ensure scripts executable
        shell: bash
        run: |
          set -euo pipefail
          chmod +x scripts/ci.sh
          chmod +x scripts/perf/suite7.sh

      - name: CI checks (fmt/clippy/test)
        shell: bash
        env:
          PECR_TEST_DB_URL: postgres://pecr:pecr@localhost:5432/pecr
        run: |
          set -euo pipefail
          mkdir -p artifacts
          ./scripts/ci.sh 2>&1 | tee artifacts/ci.log

      - name: Upload quality artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ci-quality-artifacts
          path: artifacts
          if-no-files-found: error

  perf:
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: 1.91.1

      - name: Cache cargo
        uses: Swatinem/rust-cache@v2

      - name: Ensure scripts executable
        shell: bash
        run: |
          set -euo pipefail
          chmod +x scripts/perf/suite7.sh
          chmod +x scripts/perf/compare_k6_baseline.py
          chmod +x scripts/perf/benchmark_matrix.py
          chmod +x scripts/ops/canary_rollout_guard.py

      - name: Suite 7 baseline + RLM matrix + canary guard
        shell: bash
        env:
          BASELINE_VUS: "10"
          BASELINE_DURATION: "10s"
          FAULT_VUS: "10"
          FAULT_DURATION: "5s"
          COLD_START: "1"
          WAIT_TIMEOUT_SECS: "120"
          RETRY_ATTEMPTS: "4"
          RETRY_SLEEP_SECS: "3"
          COMPOSE_PROJECT_NAME: "pecr-perf-${{ github.run_id }}-${{ github.run_attempt }}"
          CONTROLLER_P99_BUDGET_MS: "1500"
          GATEWAY_P99_BUDGET_MS: "900"
          BVR_THRESHOLD: "0.005"
          SER_THRESHOLD: "0.005"
          PECR_PERF_REGRESSION_P90_FACTOR: "1.8"
          PECR_PERF_REGRESSION_P95_FACTOR: "1.8"
          PECR_PERF_REGRESSION_P99_FACTOR: "1.8"
          PECR_PERF_REGRESSION_ABS_MS: "400"
          PECR_PERF_MIN_RATE_FACTOR: "0.85"
          PECR_PERF_RATE_ABS_DROP: "5"
          PECR_CANARY_P95_BUDGET_MS: "1200"
          PECR_CANARY_P99_BUDGET_MS: "1500"
          PECR_CANARY_SOURCE_UNAVAILABLE_MAX_RATE: "0.05"
        run: |
          set -euo pipefail
          mkdir -p artifacts artifacts/perf artifacts/ops
          overall_status=0

          set +e
          SUITE7_FAULT_EXPECT_TERMINAL_MODE="" \
          ./scripts/perf/suite7.sh 2>&1 | tee artifacts/suite7.log
          suite7_status=${PIPESTATUS[0]}
          set -e

          cp -a target/perf/. artifacts/perf/ 2>/dev/null || true
          if [[ "${suite7_status}" -ne 0 ]]; then
            overall_status="${suite7_status}"
          fi

          set +e
          PECR_CONTROLLER_ENGINE_OVERRIDE="rlm" \
          PECR_RLM_SANDBOX_ACK="1" \
          SUITE7_SKIP_FAULTS="1" \
          CONTROLLER_BASELINE_SUMMARY_NAME="suite7_rlm_baseline.summary.json" \
          GATEWAY_BASELINE_SUMMARY_NAME="suite7_rlm_gateway_baseline.summary.json" \
          METRICS_GATES_FILE="target/perf/suite7_rlm_metrics_gates.json" \
          ./scripts/perf/suite7.sh 2>&1 | tee artifacts/suite7_rlm.log
          suite7_rlm_status=${PIPESTATUS[0]}
          set -e

          cp -a target/perf/. artifacts/perf/ 2>/dev/null || true
          if [[ "${suite7_rlm_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${suite7_rlm_status}"
          fi

          set +e
          python3 scripts/perf/compare_k6_baseline.py \
            --baseline perf/baselines/suite7_baseline.summary.json \
            --current target/perf/suite7_baseline.summary.json \
            --alarm-label baseline \
            --output-json artifacts/perf/perf_alarm_baseline.json \
            --github-annotations \
            2>&1 | tee artifacts/perf_regression.log
          baseline_alarm_status=${PIPESTATUS[0]}
          set -e

          if [[ "${baseline_alarm_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${baseline_alarm_status}"
          fi

          set +e
          python3 scripts/perf/compare_k6_baseline.py \
            --baseline perf/baselines/suite7_baseline.summary.json \
            --current target/perf/suite7_rlm_baseline.summary.json \
            --alarm-label rlm \
            --output-json artifacts/perf/perf_alarm_rlm.json \
            --github-annotations \
            2>&1 | tee artifacts/perf_regression_rlm.log
          rlm_alarm_status=${PIPESTATUS[0]}
          set -e

          if [[ "${rlm_alarm_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${rlm_alarm_status}"
          fi

          set +e
          python3 scripts/perf/benchmark_matrix.py \
            --baseline perf/baselines/suite7_baseline.summary.json \
            --candidate baseline=target/perf/suite7_baseline.summary.json \
            --candidate rlm=target/perf/suite7_rlm_baseline.summary.json \
            --output-json artifacts/perf/benchmark_matrix.json \
            --output-md artifacts/perf/benchmark_matrix.md \
            --append-step-summary "${GITHUB_STEP_SUMMARY}" \
            --strict \
            2>&1 | tee artifacts/perf_matrix.log
          matrix_status=${PIPESTATUS[0]}
          set -e

          if [[ "${matrix_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${matrix_status}"
          fi

          set +e
          python3 scripts/ops/canary_rollout_guard.py \
            --summary target/perf/suite7_rlm_baseline.summary.json \
            --metrics-gates target/perf/suite7_rlm_metrics_gates.json \
            --engine rlm \
            --adaptive-enabled true \
            --batch-enabled true \
            --output-json artifacts/ops/canary_guard.json \
            --output-md artifacts/ops/canary_guard.md \
            --output-env artifacts/ops/canary_fallback.env \
            --fail-on-fallback \
            --github-annotations \
            2>&1 | tee artifacts/ops/canary_guard.log
          canary_status=${PIPESTATUS[0]}
          set -e

          if [[ "${canary_status}" -ne 0 && "${overall_status}" -eq 0 ]]; then
            overall_status="${canary_status}"
          fi

          exit "${overall_status}"

      - name: Perf stack cleanup
        if: always()
        shell: bash
        run: |
          set +e
          docker compose down -v --remove-orphans

      - name: Publish perf signal summary
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p artifacts
          baseline_summary_file="target/perf/suite7_baseline.summary.json"
          # shellcheck disable=SC2006

          {
            echo "### Suite 7 Performance Signal (Baseline + RLM)"
            echo
            if [[ -f "${baseline_summary_file}" ]]; then
          python3 - <<'PY'
          import json
          from pathlib import Path

          def print_summary(path_str: str, label: str) -> None:
              path = Path(path_str)
              if not path.exists():
                  print(f"- `{label}` summary missing: `{path_str}`")
                  return
              data = json.loads(path.read_text(encoding="utf-8"))
              metrics = data.get("metrics", {})
              dur = metrics.get("http_req_duration", {})
              p95 = dur.get("p(95)", "n/a")
              p99 = dur.get("p(99)", "n/a")
              print(f"- `{label}` p95: {p95}")
              print(f"- `{label}` p99: {p99}")

          print_summary("target/perf/suite7_baseline.summary.json", "baseline")
          print_summary("target/perf/suite7_rlm_baseline.summary.json", "rlm")
          PY
            else
              echo "- Summary JSON: missing (`target/perf/suite7_baseline.summary.json`)"
              echo "::warning::Missing Suite 7 summary JSON (target/perf/suite7_baseline.summary.json)"
            fi

            if [[ -f artifacts/perf_regression.log ]] && grep -q "perf regression gate: FAIL" artifacts/perf_regression.log; then
              echo "- Baseline regression gate: FAILED (see 'artifacts/perf_regression.log')"
              echo "::error title=Performance regression detected::Baseline regression gate failed. See artifacts/perf_regression.log."
            elif [[ -f artifacts/perf_regression.log ]]; then
              echo "- Baseline regression gate: passed"
              echo "::notice title=Performance regression gate::Baseline regression gate passed."
            else
              echo "- Baseline regression gate: no log produced"
            fi

            if [[ -f artifacts/perf_regression_rlm.log ]] && grep -q "perf regression gate: FAIL" artifacts/perf_regression_rlm.log; then
              echo "- RLM regression gate: FAILED (see 'artifacts/perf_regression_rlm.log')"
              echo "::error title=RLM regression detected::RLM regression gate failed. See artifacts/perf_regression_rlm.log."
            elif [[ -f artifacts/perf_regression_rlm.log ]]; then
              echo "- RLM regression gate: passed"
            else
              echo "- RLM regression gate: no log produced"
            fi

            if [[ -f artifacts/ops/canary_guard.json ]]; then
          python3 - <<'PY'
          import json
          from pathlib import Path

          path = Path("artifacts/ops/canary_guard.json")
          data = json.loads(path.read_text(encoding="utf-8"))
          status = data.get("status", "unknown")
          step = data.get("recommended_fallback", {}).get("step", "none")
          print(f"- Canary guard status: {status} (recommended step: {step})")
          PY
            else
              echo "- Canary guard: artifact missing (`artifacts/ops/canary_guard.json`)"
            fi

            echo "- Alarm artifacts: 'artifacts/perf/perf_alarm_baseline.json', 'artifacts/perf/perf_alarm_rlm.json'"
            echo "- Benchmark matrix: 'artifacts/perf/benchmark_matrix.md'"
            echo "- Canary fallback env patch: 'artifacts/ops/canary_fallback.env'"
          } | tee artifacts/perf_signal_summary.md >> "${GITHUB_STEP_SUMMARY}"

      - name: Upload perf artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ci-perf-artifacts
          path: artifacts
          if-no-files-found: error

  contracts:
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Contract lock check
        run: python3 scripts/contracts/check_contract_lock.py

  gate:
    if: always()
    needs:
      - quality
      - perf
      - contracts
    runs-on: ubuntu-latest
    steps:
      - name: Enforce all CI gates passed
        run: |
          set -euo pipefail
          test "${{ needs.quality.result }}" = "success"
          test "${{ needs.perf.result }}" = "success"
          test "${{ needs.contracts.result }}" = "success"
